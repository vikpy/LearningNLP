{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "NLP Visualizing tweets and the logistic regression model",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vikpy/LearningNLP/blob/master/NLP_Visualizing_tweets_and_the_logistic_regression_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikkJVaZTcevx",
        "colab_type": "text"
      },
      "source": [
        "# Visualizing tweets and the Logistic Regression model\n",
        "\n",
        "**Objectives:** Visualize and interpret the logistic regression model\n",
        "\n",
        "**Steps:**\n",
        "* Plot tweets in a scatter plot using their positive and negative sums.\n",
        "* Plot the output of the logistic regression model in the same plot as a solid line"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzUMNC6Jcev1",
        "colab_type": "text"
      },
      "source": [
        "## Import the required libraries\n",
        "\n",
        "We will be using [*NLTK*](http://www.nltk.org/howto/twitter.html), an opensource NLP library, for collecting, handling, and processing Twitter data. In this lab, we will use the example dataset that comes alongside with NLTK. This dataset has been manually annotated and serves to establish baselines for models quickly. \n",
        "\n",
        "So, to start, let's import the required libraries. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mADmUANefRmG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "import re as re\n",
        "def build_freqs(tweets, ys):\n",
        "    freqs = {}\n",
        "    yslist = np.squeeze(ys).tolist()\n",
        "    for y, tweet in zip(yslist, tweets):\n",
        "        for word in process_tweet(tweet):\n",
        "            pair = (word, y)\n",
        "            if pair in freqs:\n",
        "                freqs[pair] += 1\n",
        "            else:\n",
        "                freqs[pair] = 1\n",
        "    return freqs\n",
        "\n",
        "def extract_features(tweet, freqs,flag):\n",
        "    word_l = process_tweet(tweet)\n",
        "    x = np.zeros((1, 4)) \n",
        "    x[0,0] = 1\n",
        "       \n",
        "    if flag == 1:\n",
        "      x[0,3]=1\n",
        "    else:\n",
        "      x[0,3]=0\n",
        "        \n",
        "    for word in word_l:\n",
        "      x[0,1] += len([1 for key, value in freqs.items() if word==key[0]and key[1]==1])\n",
        "      x[0,2] += len([1 for key, value in freqs.items() if word==key[0]and key[1]==0])\n",
        "        \n",
        "    return x\n",
        "\n",
        "def process_tweet(tweet):\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    tokenizer = TweetTokenizer(preserve_case=False,        strip_handles=True,reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and  \n",
        "                word not in string.punctuation): \n",
        "            stem_word = stemmer.stem(word)  # stemming word\n",
        "            tweets_clean.append(stem_word)\n",
        "\n",
        "    return tweets_clean\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_6pd9v3cev4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "f89331bf-da47-4ce2-b965-eb793b1e50d7"
      },
      "source": [
        "import nltk                         # NLP toolbox\n",
        "from os import getcwd\n",
        "import pandas as pd                 # Library for Dataframes \n",
        "from nltk.corpus import twitter_samples \n",
        "import matplotlib.pyplot as plt     # Library for visualization\n",
        "import numpy as np                  # Library for math functions\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('twitter_samples')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXNvrlP4cewE",
        "colab_type": "text"
      },
      "source": [
        "## Load the NLTK sample dataset\n",
        "\n",
        "To complete this lab, you need the sample dataset of the previous lab. Here, we assume the files are already available, and we only need to load into Python lists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef9gwswLcewG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e514d94b-bae4-41a0-9d34-ad3131ee6675"
      },
      "source": [
        "# select the set of positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "tweets = all_positive_tweets + all_negative_tweets ## Concatenate the lists. \n",
        "labels = np.append(np.ones((len(all_positive_tweets),1)), np.zeros((len(all_negative_tweets),1)), axis = 0)\n",
        "\n",
        "# split the data into two pieces, one for training and one for testing (validation set) \n",
        "train_pos  = all_positive_tweets[:4000]\n",
        "train_neg  = all_negative_tweets[:4000]\n",
        "\n",
        "train_x = train_pos + train_neg \n",
        "\n",
        "print(\"Number of tweets: \", len(train_x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of tweets:  8000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic7VfOyacewQ",
        "colab_type": "text"
      },
      "source": [
        "# Load the extracted features\n",
        "\n",
        "Part of this week's assignment is the creation of the numerical features needed for the Logistic regression model. In order not to interfere with it, we have previously calculated and stored these features in a CSV file for the entire training set.\n",
        "\n",
        "So, please load these features created for the tweets sample. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzgESTPGcewR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "7c0aa0c4-1804-4b95-d01c-fad31ea7fa8f"
      },
      "source": [
        "data = pd.read_csv('logistic_features.csv'); # Load a 3 columns csv file using pandas function\n",
        "data.head(10) # Print the first 10 data entries"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-a28b55f901bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'logistic_features.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;31m# Load a 3 columns csv file using pandas function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Print the first 10 data entries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File logistic_features.csv does not exist: 'logistic_features.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2AgV5CMcewV",
        "colab_type": "text"
      },
      "source": [
        "Now let us get rid of the data frame to keep only Numpy arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgpZ9_ZEcewW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Each feature is labeled as bias, positive and negative\n",
        "X = data[['bias', 'positive', 'negative']].values # Get only the numerical values of the dataframe\n",
        "Y = data['sentiment'].values; # Put in Y the corresponding labels or sentiments\n",
        "\n",
        "print(X.shape) # Print the shape of the X part\n",
        "print(X) # Print some rows of X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH-MB2UZcewe",
        "colab_type": "text"
      },
      "source": [
        "## Load a pretrained Logistic Regression model\n",
        "\n",
        "In the same way, as part of this week's assignment, a Logistic regression model must be trained. The next cell contains the resulting model from such training. Notice that a list of 3 numeric values represents the whole model, that we have called _theta_ $\\theta$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC3vTkywcewf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "theta = [7e-08, 0.0005239, -0.00055517]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfMd6eZwcewl",
        "colab_type": "text"
      },
      "source": [
        "## Plot the samples in a scatter plot\n",
        "\n",
        "The vector theta represents a plane that split our feature space into two parts. Samples located over that plane are considered positive, and samples located under that plane are considered negative. Remember that we have a 3D feature space, i.e., each tweet is represented as a vector comprised of three values: `[bias, positive_sum, negative_sum]`, always having `bias = 1`. \n",
        "\n",
        "If we ignore the bias term, we can plot each tweet in a cartesian plane, using `positive_sum` and `negative_sum`. In the cell below, we do precisely this. Additionally, we color each tweet, depending on its class. Positive tweets will be green and negative tweets will be red."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKx-C4focewl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the samples using columns 1 and 2 of the matrix\n",
        "fig, ax = plt.subplots(figsize = (8, 8))\n",
        "\n",
        "colors = ['red', 'green']\n",
        "\n",
        "# Color based on the sentiment Y\n",
        "ax.scatter(X[:,1], X[:,2], c=[colors[int(k)] for k in Y], s = 0.1)  # Plot a dot for each pair of words\n",
        "plt.xlabel(\"Positive\")\n",
        "plt.ylabel(\"Negative\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0cH2jpacewq",
        "colab_type": "text"
      },
      "source": [
        "From the plot, it is evident that the features that we have chosen to represent tweets as numerical vectors allow an almost perfect separation between positive and negative tweets. So you can expect a very high accuracy for this model! \n",
        "\n",
        "## Plot the model alongside the data\n",
        "\n",
        "We will draw a gray line to show the cutoff between the positive and negative regions. In other words, the gray line marks the line where $$ z = \\theta * x = 0.$$\n",
        "To draw this line, we have to solve the above equation in terms of one of the independent variables.\n",
        "\n",
        "$$ z = \\theta * x = 0$$\n",
        "$$ x = [1, pos, neg] $$\n",
        "$$ z(\\theta, x) = \\theta_0+ \\theta_1 * pos + \\theta_2 * neg = 0 $$\n",
        "$$ neg = (-\\theta_0 - \\theta_1 * pos) / \\theta_2 $$\n",
        "\n",
        "The red and green lines that point in the direction of the corresponding sentiment are calculated using a perpendicular line to the separation line calculated in the previous equations(neg function). It must point in the same direction as the derivative of the Logit function, but the magnitude may differ. It is only for a visual representation of the model. \n",
        "\n",
        "$$direction = pos * \\theta_2 / \\theta_1$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8MhkyX7cewr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Equation for the separation plane\n",
        "# It give a value in the negative axe as a function of a positive value\n",
        "# f(pos, neg, W) = w0 + w1 * pos + w2 * neg = 0\n",
        "# s(pos, W) = (w0 - w1 * pos) / w2\n",
        "def neg(theta, pos):\n",
        "    return (-theta[0] - pos * theta[1]) / theta[2]\n",
        "\n",
        "# Equation for the direction of the sentiments change\n",
        "# We don't care about the magnitude of the change. We are only interested \n",
        "# in the direction. So this direction is just a perpendicular function to the \n",
        "# separation plane\n",
        "# df(pos, W) = pos * w2 / w1\n",
        "def direction(theta, pos):\n",
        "    return    pos * theta[2] / theta[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_KoB-qGcewx",
        "colab_type": "text"
      },
      "source": [
        "The green line in the chart points in the direction where z > 0 and the red line points in the direction where z < 0. The direction of these lines are given by the weights $\\theta_1$ and $\\theta_2$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uurdBo3cewx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the samples using columns 1 and 2 of the matrix\n",
        "fig, ax = plt.subplots(figsize = (8, 8))\n",
        "\n",
        "colors = ['red', 'green']\n",
        "\n",
        "# Color base on the sentiment Y\n",
        "ax.scatter(X[:,1], X[:,2], c=[colors[int(k)] for k in Y], s = 0.1)  # Plot a dot for each pair of words\n",
        "plt.xlabel(\"Positive\")\n",
        "plt.ylabel(\"Negative\")\n",
        "\n",
        "# Now lets represent the logistic regression model in this chart. \n",
        "maxpos = np.max(X[:,1])\n",
        "\n",
        "offset = 5000 # The pos value for the direction vectors origin\n",
        "\n",
        "# Plot a gray line that divides the 2 areas.\n",
        "ax.plot([0,  maxpos], [neg(theta, 0),   neg(theta, maxpos)], color = 'gray') \n",
        "\n",
        "# Plot a green line pointing to the positive direction\n",
        "ax.arrow(offset, neg(theta, offset), offset, direction(theta, offset), head_width=500, head_length=500, fc='g', ec='g')\n",
        "# Plot a red line pointing to the negative direction\n",
        "ax.arrow(offset, neg(theta, offset), -offset, -direction(theta, offset), head_width=500, head_length=500, fc='r', ec='r')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNAQGtYbcew2",
        "colab_type": "text"
      },
      "source": [
        "**Note that more critical than the Logistic regression itself, are the features extracted from tweets that allow getting the right results in this exercise.**\n",
        "\n",
        "That is all, folks. Hopefully, now you understand better what the Logistic regression model represents, and why it works that well for this specific problem. "
      ]
    }
  ]
}